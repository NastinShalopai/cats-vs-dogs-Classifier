{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f41fb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision as tv\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c1d4726",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eda52eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e34e259b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset2class(torch.utils.data.Dataset):\n",
    "    def __init__(self, path_dir1:str, path_dir2:str):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.path_dir1 = path_dir1\n",
    "        self.path_dir2 = path_dir2\n",
    "        \n",
    "        self.list_dir1 = sorted(os.listdir(path_dir1))\n",
    "        self.list_dir2 = sorted(os.listdir(path_dir2))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.list_dir1) + len(self.list_dir2)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if idx < len(self.list_dir1):\n",
    "            class_id = 0\n",
    "            img_path = os.path.join(self.path_dir1, self.list_dir1[idx])\n",
    "        else:\n",
    "            class_id = 1\n",
    "            idx -= len(self.list_dir1)\n",
    "            img_path = os.path.join(self.path_dir2, self.list_dir2[idx])\n",
    "            \n",
    "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = img.astype(np.float32)/255.0\n",
    "        img = cv2.resize(img, (224, 224), interpolation=cv2.INTER_AREA)\n",
    "        img = img.transpose((2, 0, 1))\n",
    "        \n",
    "        t_img = torch.from_numpy(img)\n",
    "        \n",
    "        t_class_id = torch.tensor(class_id)\n",
    "        \n",
    "        return (t_img, class_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c75722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dogs_path = 'path",
    "data_cats_path = 'path",
    "\n",
    "dataset_catvsdog = Dataset2class(data_cats_path, data_dogs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e20c592",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = torch.utils.data.random_split(dataset_catvsdog, [0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4afd4cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "shuffle = True\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=shuffle,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset=val_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=shuffle,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "752d5a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "    def __init__(self, out_nc):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.flat = nn.Flatten()\n",
    "        \n",
    "        self.fc1 = nn.Linear(7*7*512, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc3 = nn.Linear(4096, out_nc)\n",
    "        \n",
    "        self.BatchNorm = nn.BatchNorm1d(4096)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv1_1(x)\n",
    "        out = self.act(out)\n",
    "        out = self.conv1_2(out)\n",
    "        out = self.act(out)\n",
    "        \n",
    "        out = self.maxpool(out)\n",
    "\n",
    "        out = self.conv2_1(out)\n",
    "        out = self.act(out)\n",
    "        out = self.conv2_2(out)\n",
    "        out = self.act(out)\n",
    "        \n",
    "        out = self.maxpool(out)\n",
    "        \n",
    "        out = self.conv3_1(out)\n",
    "        out = self.act(out)\n",
    "        out = self.conv3_2(out)\n",
    "        out = self.act(out)\n",
    "        out = self.conv3_3(out)\n",
    "        out = self.act(out)\n",
    "        \n",
    "        out = self.maxpool(out)\n",
    "        \n",
    "        out = self.conv4_1(out)\n",
    "        out = self.act(out)\n",
    "        out = self.conv4_2(out)\n",
    "        out = self.act(out)\n",
    "        out = self.conv4_3(out)\n",
    "        out = self.act(out)\n",
    "        \n",
    "        out = self.maxpool(out)\n",
    "        \n",
    "        out = self.conv5_1(out)\n",
    "        out = self.act(out)\n",
    "        out = self.conv5_2(out)\n",
    "        out = self.act(out)\n",
    "        out = self.conv5_3(out)\n",
    "        out = self.act(out)\n",
    "        \n",
    "        out = self.maxpool(out)\n",
    "        \n",
    "        out = self.flat(out)\n",
    "        \n",
    "        out = self.fc1(out)\n",
    "        out = self.BatchNorm(out)\n",
    "        out = self.act(out) \n",
    "        out = self.fc2(out)\n",
    "        out = self.BatchNorm(out)\n",
    "        out = self.act(out)\n",
    "        out = self.fc3(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fc59b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16(2)\n",
    "# model = torch.load('4XConv+4XMaxpool+2Xlieear_Model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e60c8559",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aae8d2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, epoch):\n",
    "    total_loss=0\n",
    "    correct=0\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        \n",
    "        X, y = X.cuda(), y.cuda()\n",
    "        \n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "        tensor_board.add_scalar('Train_loss', total_loss, epoch)\n",
    "        tensor_board.add_scalar('Test_accuracy', correct / size, epoch)\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn, max_test_accuracy, epoch):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.cuda(), y.cuda()\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    \n",
    "    tensor_board.add_scalar('Test_loss', test_loss, epoch)\n",
    "    tensor_board.add_scalar('Test_accuracy', correct / size, epoch)\n",
    "    \n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    \n",
    "    if correct < max_test_accuracy:\n",
    "        return max_test_accuracy\n",
    "    else:\n",
    "        model_file_name = '4XConv+4XMaxpool+2Xlieear_Model.pth'\n",
    "        print(f'New Accuracy record: {(100*max_test_accuracy):>0.1f}% ----> '\n",
    "              f'{(100*correct):>0.1f}% saving new model {model_file_name}')\n",
    "        torch.save(model, model_file_name)\n",
    "        return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7802b2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "tensor_board = SummaryWriter(comment = f\"Batch size={batch_size} Learning rate={learning_rate} Shuffle={shuffle}\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(params = model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22773361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cuda')\n",
    "loss_fn.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4cfe006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.083552  [   32/19957]\n",
      "loss: 0.078578  [ 3232/19957]\n",
      "loss: 0.038333  [ 6432/19957]\n",
      "loss: 0.206599  [ 9632/19957]\n",
      "loss: 0.125698  [12832/19957]\n",
      "loss: 0.010968  [16032/19957]\n",
      "loss: 0.072404  [19232/19957]\n",
      "Test Error: \n",
      " Accuracy: 96.4%, Avg loss: 0.088371 \n",
      "\n",
      "New Accuracy record: 0.0% ----> 96.4% saving new model 4XConv+4XMaxpool+2Xlieear_Model.pth\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.026345  [   32/19957]\n",
      "loss: 0.011585  [ 3232/19957]\n",
      "loss: 0.023104  [ 6432/19957]\n",
      "loss: 0.009080  [ 9632/19957]\n",
      "loss: 0.024538  [12832/19957]\n",
      "loss: 0.006950  [16032/19957]\n",
      "loss: 0.025009  [19232/19957]\n",
      "Test Error: \n",
      " Accuracy: 96.7%, Avg loss: 0.087901 \n",
      "\n",
      "New Accuracy record: 96.4% ----> 96.7% saving new model 4XConv+4XMaxpool+2Xlieear_Model.pth\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.025097  [   32/19957]\n",
      "loss: 0.044253  [ 3232/19957]\n",
      "loss: 0.035673  [ 6432/19957]\n",
      "loss: 0.019586  [ 9632/19957]\n",
      "loss: 0.030367  [12832/19957]\n",
      "loss: 0.225238  [16032/19957]\n",
      "loss: 0.024003  [19232/19957]\n",
      "Test Error: \n",
      " Accuracy: 96.2%, Avg loss: 0.099748 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.015379  [   32/19957]\n",
      "loss: 0.021936  [ 3232/19957]\n",
      "loss: 0.072255  [ 6432/19957]\n",
      "loss: 0.290453  [ 9632/19957]\n",
      "loss: 0.037656  [12832/19957]\n",
      "loss: 0.031155  [16032/19957]\n",
      "loss: 0.104718  [19232/19957]\n",
      "Test Error: \n",
      " Accuracy: 96.8%, Avg loss: 0.087724 \n",
      "\n",
      "New Accuracy record: 96.7% ----> 96.8% saving new model 4XConv+4XMaxpool+2Xlieear_Model.pth\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.052548  [   32/19957]\n",
      "loss: 0.002353  [ 3232/19957]\n",
      "loss: 0.013642  [ 6432/19957]\n",
      "loss: 0.005060  [ 9632/19957]\n",
      "loss: 0.075973  [12832/19957]\n",
      "loss: 0.016476  [16032/19957]\n",
      "loss: 0.045267  [19232/19957]\n",
      "Test Error: \n",
      " Accuracy: 95.8%, Avg loss: 0.109117 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "max_test_accuracy = 0\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_loader, model, loss_fn, optimizer, epoch=t+1)\n",
    "    max_test_accuracy = test_loop(val_loader, model, loss_fn, max_test_accuracy, epoch=t+1)\n",
    "print(\"Done!\")\n",
    "tensor_board.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb357f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
